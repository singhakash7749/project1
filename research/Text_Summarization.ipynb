{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/akash/.local/lib/python3.8/site-packages (from accelerate) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/akash/.local/lib/python3.8/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in /home/akash/.local/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/akash/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.6.0)\n",
      "Requirement already satisfied: sympy in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/akash/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/akash/.local/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.65.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/akash/.local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Found existing installation: transformers 4.37.1\n",
      "Uninstalling transformers-4.37.1:\n",
      "  Successfully uninstalled transformers-4.37.1\n",
      "Found existing installation: accelerate 0.26.1\n",
      "Uninstalling accelerate-0.26.1:\n",
      "  Successfully uninstalled accelerate-0.26.1\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/akash/.local/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/akash/.local/lib/python3.8/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/akash/.local/lib/python3.8/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/akash/.local/lib/python3.8/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: psutil in /home/akash/.local/lib/python3.8/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/akash/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.6.0)\n",
      "Requirement already satisfied: sympy in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/akash/.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from requests->transformers) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/akash/.local/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Using cached transformers-4.37.1-py3-none-any.whl (8.4 MB)\n",
      "Using cached accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate, transformers\n",
      "Successfully installed accelerate-0.26.1 transformers-4.37.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade accelerate\n",
    "!pip uninstall -y transformers accelerate\n",
    "!pip install transformers accelerate\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/akash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-27 22:10:56--  https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
      "Resolving github.com (github.com)... 20.207.73.82\n",
      "Connecting to github.com (github.com)|20.207.73.82|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip [following]\n",
      "--2024-01-27 22:10:56--  https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7903594 (7.5M) [application/zip]\n",
      "Saving to: ‘summarizer-data.zip.2’\n",
      "\n",
      "summarizer-data.zip 100%[===================>]   7.54M  1.29MB/s    in 6.3s    \n",
      "\n",
      "2024-01-27 22:11:03 (1.21 MB/s) - ‘summarizer-data.zip.2’ saved [7903594/7903594]\n",
      "\n",
      "Archive:  summarizer-data.zip\n",
      "replace samsum-test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
    "!unzip summarizer-data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum = load_from_disk('samsum_dataset')\n",
    "dataset_samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split lengths: [14732, 819, 818]\n",
      "Features: ['id', 'dialogue', 'summary']\n",
      "\n",
      "Dialogue:\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Summary:\n",
      "Eric and Rob are going to watch a stand-up on youtube.\n"
     ]
    }
   ],
   "source": [
    "split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n",
    "\n",
    "print(f\"split lengths: {split_lengths}\")\n",
    "print(f\"Features: {dataset_samsum['train'].column_names}\")\n",
    "print(\"\\nDialogue:\")\n",
    "print(dataset_samsum[\"test\"][1][\"dialogue\"])\n",
    "print(\"\\nSummary:\")\n",
    "print(dataset_samsum[\"test\"][1][\"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]/home/akash/anaconda3/envs/textsum/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:3866: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 14732/14732 [00:02<00:00, 5665.49 examples/s]\n",
      "Map: 100%|██████████| 819/819 [00:00<00:00, 4185.16 examples/s]\n",
      "Map: 100%|██████████| 818/818 [00:00<00:00, 5974.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 14732\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_samsum_pt[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n",
    "    per_device_train_batch_size=1, per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, logging_steps=10,\n",
    "    evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model_pegasus, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_samsum_pt[\"test\"],\n",
    "                  eval_dataset=dataset_samsum_pt[\"validation\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
